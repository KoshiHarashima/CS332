{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "os.makedirs('../../figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f49872",
   "metadata": {},
   "source": [
    "- what to do\n",
    "    - give raphs for each learning rate\n",
    "    - the change of regret?, total payoff, decisions?\n",
    "    - horizontal axis must be the number of rounds\n",
    "    - vertical axis must be one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066662b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions (k): 10\n",
      "Number of rounds (n): 1000\n",
      "Random epsilon: 0.001000\n",
      "Theoretical optimal epsilon: 0.047985\n",
      "FTL epsilon: 1000\n"
     ]
    }
   ],
   "source": [
    "# Define variables and learning rates\n",
    "k = 10  # actions\n",
    "n = 1000  #  rounds\n",
    "\n",
    "# Three important learning rates to compare:\n",
    "# 1. Random \n",
    "epsilon_random = 0.001  \n",
    "\n",
    "# 2. Theoretical optimal\n",
    "epsilon_optimal = np.sqrt(np.log(k) / n)\n",
    "\n",
    "# 3. FTL\n",
    "epsilon_ftl = 1000  \n",
    "\n",
    "print(f\"Number of actions (k): {k}\")\n",
    "print(f\"Number of rounds (n): {n}\")\n",
    "print(f\"Random epsilon: {epsilon_random:.6f}\")\n",
    "print(f\"Theoretical optimal epsilon: {epsilon_optimal:.6f}\")\n",
    "print(f\"FTL epsilon: {epsilon_ftl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aeec014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Exponential Weights Algorithm\n",
    "class ExponentialWeights:\n",
    "    # Three Parameters:\n",
    "    \n",
    "    def __init__(self, k, epsilon, n):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.n = n\n",
    "        self.weights = np.ones(k)  # Initialize weights to 1\n",
    "        self.cumulative_payoffs = np.zeros(k)  # Track cumulative payoffs\n",
    "        self.regret_history = []  # Track regret over time\n",
    "        self.total_payoff = 0  # Track total payoff\n",
    "        self.action_history = []  # Track actions taken\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action based on current weights\"\"\"\n",
    "        if self.epsilon == 0 or np.all(self.weights == 0):\n",
    "            # Random selection when epsilon is 0\n",
    "            action = np.random.randint(0, self.k)\n",
    "        else:\n",
    "            # Normalize weights to get probabilities\n",
    "            probabilities = self.weights / np.sum(self.weights)\n",
    "            action = np.random.choice(self.k, p=probabilities)\n",
    "        \n",
    "        self.action_history.append(action)\n",
    "        return action\n",
    "    \n",
    "    def update_weights(self, payoffs):\n",
    "        \"\"\"\n",
    "        Update weights based on received payoffs\n",
    "        \n",
    "        Parameters:\n",
    "        - payoffs: array of payoffs for each action in this round\n",
    "        \"\"\"\n",
    "        # Update cumulative payoffs\n",
    "        self.cumulative_payoffs += payoffs\n",
    "        \n",
    "        # Update total payoff (payoff of selected action)\n",
    "        selected_action = self.action_history[-1]\n",
    "        self.total_payoff += payoffs[selected_action]\n",
    "        \n",
    "        # Update weights: w_i = w_i * exp(epsilon * payoff_i)\n",
    "        if self.epsilon > 0:\n",
    "            self.weights *= np.exp(self.epsilon * payoffs)\n",
    "        \n",
    "        # Calculate regret: max cumulative payoff - our cumulative payoff\n",
    "        max_cumulative = np.max(self.cumulative_payoffs)\n",
    "        our_cumulative = self.cumulative_payoffs[selected_action]\n",
    "        regret = max_cumulative - our_cumulative\n",
    "        self.regret_history.append(regret)\n",
    "    \n",
    "    def run_algorithm(self, payoff_generator):\n",
    "        \"\"\"\n",
    "        Run the algorithm for n rounds\n",
    "        \n",
    "        Parameters:\n",
    "        - payoff_generator: function that generates payoffs for each round\n",
    "        \"\"\"\n",
    "        for round_num in range(self.n):\n",
    "            # Select action\n",
    "            action = self.select_action()\n",
    "            \n",
    "            # Generate payoffs for this round\n",
    "            payoffs = payoff_generator(round_num)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(payoffs)\n",
    "        \n",
    "        return {\n",
    "            'regret_history': self.regret_history,\n",
    "            'total_payoff': self.total_payoff,\n",
    "            'action_history': self.action_history,\n",
    "            'cumulative_payoffs': self.cumulative_payoffs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fdb29",
   "metadata": {},
   "source": [
    "# A. Adversarial Fair Payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bea05d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Adversarial Fair Payoffs Model...\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Create payoff generator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m adversarial_generator = AdversarialFairPayoffs(\u001b[43mk\u001b[49m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Test with different learning rates\u001b[39;00m\n\u001b[32m     44\u001b[39m ew_random_adv = ExponentialWeights(k, epsilon_random, n)\n",
      "\u001b[31mNameError\u001b[39m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "# A. Adversarial Fair Payoffs Implementation\n",
    "class AdversarialFairPayoffs:\n",
    "    \"\"\"\n",
    "    Adversarial Fair Payoffs model:\n",
    "    - In each round, draw a payoff x ~ U[0,1]\n",
    "    - Assign this payoff to the action with smallest total payoff so far\n",
    "    - All other actions get 0 payoff\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.cumulative_payoffs = np.zeros(k)\n",
    "    \n",
    "    def generate_payoffs(self, round_num):\n",
    "        \"\"\"\n",
    "        Generate payoffs for a given round\n",
    "        \n",
    "        Returns:\n",
    "        - payoffs: array of payoffs for each action\n",
    "        \"\"\"\n",
    "        # Draw a random payoff from uniform distribution [0,1]\n",
    "        payoff = np.random.uniform(0, 1)\n",
    "        \n",
    "        # Find the action with smallest cumulative payoff\n",
    "        min_action = np.argmin(self.cumulative_payoffs)\n",
    "        \n",
    "        # Create payoff vector: only the min action gets the payoff, others get 0\n",
    "        payoffs = np.zeros(self.k)\n",
    "        payoffs[min_action] = payoff\n",
    "        \n",
    "        # Update cumulative payoffs\n",
    "        self.cumulative_payoffs += payoffs\n",
    "        \n",
    "        return payoffs\n",
    "\n",
    "# Test Adversarial Fair Payoffs\n",
    "print(\"Testing Adversarial Fair Payoffs Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create payoff generator\n",
    "adversarial_generator = AdversarialFairPayoffs(k)\n",
    "\n",
    "# Test with different learning rates\n",
    "ew_random_adv = ExponentialWeights(k, epsilon_random, n)\n",
    "ew_optimal_adv = ExponentialWeights(k, epsilon_optimal, n)\n",
    "ew_ftl_adv = ExponentialWeights(k, epsilon_ftl, n)\n",
    "\n",
    "# Run algorithms\n",
    "results_random_adv = ew_random_adv.run_algorithm(adversarial_generator.generate_payoffs)\n",
    "results_optimal_adv = ew_optimal_adv.run_algorithm(adversarial_generator.generate_payoffs)\n",
    "results_ftl_adv = ew_ftl_adv.run_algorithm(adversarial_generator.generate_payoffs)\n",
    "\n",
    "# Create results dictionary for adversarial payoffs\n",
    "results_dict_adv = {\n",
    "    f'Random (ε={epsilon_random:.6f})': results_random_adv,\n",
    "    f'Optimal (ε={epsilon_optimal:.6f})': results_optimal_adv,\n",
    "    f'FTL (ε={epsilon_ftl})': results_ftl_adv\n",
    "}\n",
    "\n",
    "# Plot comparison for adversarial payoffs\n",
    "plot_learning_rate_comparison(results_dict_adv, \"Exponential Weights: Learning Rate Comparison (Adversarial Fair Payoffs)\")\n",
    "\n",
    "# Print summary statistics for adversarial payoffs\n",
    "print(\"\\nAdversarial Fair Payoffs - Summary Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for name, results in results_dict_adv.items():\n",
    "    final_regret = results['regret_history'][-1]\n",
    "    total_payoff = results['total_payoff']\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Final Regret: {final_regret:.4f}\")\n",
    "    print(f\"  Total Payoff: {total_payoff:.4f}\")\n",
    "    print(f\"  Best Action: Action {np.argmax(results['cumulative_payoffs'])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3889ad",
   "metadata": {},
   "source": [
    "# B. Bernoulli Payoffs Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4709cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Bernoulli Payoffs Model...\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Create payoff generator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m bernoulli_generator = BernoulliPayoffs(\u001b[43mk\u001b[49m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Test with different learning rates\u001b[39;00m\n\u001b[32m     33\u001b[39m ew_random_bern = ExponentialWeights(k, epsilon_random, n)\n",
      "\u001b[31mNameError\u001b[39m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "class BernoulliPayoffs:\n",
    "    \"\"\"\n",
    "    Bernoulli Payoffs model:\n",
    "    - Fix a probability p_j for each action j with p_j in [0, 1/2]\n",
    "    - In each round, draw payoff for each action j as v_j ~ B(p_j)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        # Generate probabilities for each action (all in [0, 1/2])\n",
    "        self.probabilities = np.random.uniform(0, 0.5, k)\n",
    "        print(f\"Bernoulli probabilities for each action: {self.probabilities}\")\n",
    "    \n",
    "    def generate_payoffs(self, round_num):\n",
    "        \"\"\"\n",
    "        Generate payoffs for a given round\n",
    "        \n",
    "        Returns:\n",
    "        - payoffs: array of payoffs for each action (0 or 1)\n",
    "        \"\"\"\n",
    "        # Generate Bernoulli payoffs for each action\n",
    "        payoffs = np.random.binomial(1, self.probabilities)\n",
    "        return payoffs\n",
    "\n",
    "# Test Bernoulli Payoffs\n",
    "print(\"\\nTesting Bernoulli Payoffs Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create payoff generator\n",
    "bernoulli_generator = BernoulliPayoffs(k)\n",
    "\n",
    "# Test with different learning rates\n",
    "ew_random_bern = ExponentialWeights(k, epsilon_random, n)\n",
    "ew_optimal_bern = ExponentialWeights(k, epsilon_optimal, n)\n",
    "ew_ftl_bern = ExponentialWeights(k, epsilon_ftl, n)\n",
    "\n",
    "# Run algorithms\n",
    "results_random_bern = ew_random_bern.run_algorithm(bernoulli_generator.generate_payoffs)\n",
    "results_optimal_bern = ew_optimal_bern.run_algorithm(bernoulli_generator.generate_payoffs)\n",
    "results_ftl_bern = ew_ftl_bern.run_algorithm(bernoulli_generator.generate_payoffs)\n",
    "\n",
    "# Create results dictionary for Bernoulli payoffs\n",
    "results_dict_bern = {\n",
    "    f'Random (ε={epsilon_random:.6f})': results_random_bern,\n",
    "    f'Optimal (ε={epsilon_optimal:.6f})': results_optimal_bern,\n",
    "    f'FTL (ε={epsilon_ftl})': results_ftl_bern\n",
    "}\n",
    "\n",
    "# Plot comparison for Bernoulli payoffs\n",
    "plot_learning_rate_comparison(results_dict_bern, \"Exponential Weights: Learning Rate Comparison (Bernoulli Payoffs)\")\n",
    "\n",
    "# Print summary statistics for Bernoulli payoffs\n",
    "print(\"\\nBernoulli Payoffs - Summary Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for name, results in results_dict_bern.items():\n",
    "    final_regret = results['regret_history'][-1]\n",
    "    total_payoff = results['total_payoff']\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Final Regret: {final_regret:.4f}\")\n",
    "    print(f\"  Total Payoff: {total_payoff:.4f}\")\n",
    "    print(f\"  Best Action: Action {np.argmax(results['cumulative_payoffs'])}\")\n",
    "    print()\n",
    "\n",
    "# Show the true probabilities for reference\n",
    "print(\"True Bernoulli probabilities for each action:\")\n",
    "for i, prob in enumerate(bernoulli_generator.probabilities):\n",
    "    print(f\"  Action {i}: {prob:.4f}\")\n",
    "print(f\"Best action (highest probability): Action {np.argmax(bernoulli_generator.probabilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e775670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "670ba2b4",
   "metadata": {},
   "source": [
    "# Data in the wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e3713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa39dde",
   "metadata": {},
   "source": [
    "# Adversarial generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6d00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
