{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths dynamically\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'code' else current_dir\n",
    "code_dir = project_root / 'code'\n",
    "data_dir = project_root / 'data'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Add code directory to Python path\n",
    "sys.path.append(str(code_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Code directory: {code_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Figures directory: {figures_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "from A_afp import AdversarialFairPayoffs\n",
    "from B_bp import BernoulliPayoffs\n",
    "# from C_pp import PachinkoPayoffs\n",
    "# from D_ad import AdversarialGenerativeModel\n",
    "from EW import ExponentialWeightsAlgorithm\n",
    "from MC import monte_carlo_simulation\n",
    "\n",
    "print(\"All modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "k = 3  \n",
    "n = 1000  \n",
    "num_simulations = 100  # Monte Carlo simulation times\n",
    "\n",
    "# Epsilon values\n",
    "epsilon_values = {\n",
    "    'random': 0.01,  \n",
    "    'optimal': np.sqrt(np.log(k) / n), \n",
    "    'FTL': 100  \n",
    "}\n",
    "\n",
    "print(f\"Epsilon values:\")\n",
    "for name, value in epsilon_values.items():\n",
    "    print(f\"  {name}: {value:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdversarialFairPayoffs environment\n",
    "\n",
    "# Save results\n",
    "results_data = []\n",
    "\n",
    "for epsilon_name, epsilon_value in epsilon_values.items():\n",
    "    print(f\"\\nRunning simulations for {epsilon_name} (epsilon = {epsilon_value:.6f})...\")\n",
    "    \n",
    "    regret_histories = []\n",
    "    total_payoffs = []\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        env = AdversarialFairPayoffs(k)\n",
    "        algorithm = ExponentialWeightsAlgorithm(k, epsilon=epsilon_value, n=n)\n",
    "        \n",
    "        # Run algorithm\n",
    "        regret_history, total_payoff, action_history, cumulative_payoffs = algorithm.run_algorithm(env.generate_payoffs)\n",
    "        \n",
    "        regret_histories.append(regret_history)\n",
    "        total_payoffs.append(total_payoff)\n",
    "        \n",
    "        if (sim + 1) % 20 == 0:\n",
    "            print(f\"  Completed {sim + 1}/{num_simulations} simulations\")\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    regret_array = np.array(regret_histories)\n",
    "    mean_regret = np.mean(regret_array, axis=0)\n",
    "    std_regret = np.std(regret_array, axis=0)\n",
    "    final_regrets = regret_array[:, -1]\n",
    "    \n",
    "    for round_num in range(n):\n",
    "        results_data.append({\n",
    "            'round': round_num + 1,\n",
    "            'epsilon_type': epsilon_name,\n",
    "            'epsilon_value': epsilon_value,\n",
    "            'mean_regret': mean_regret[round_num],\n",
    "            'std_regret': std_regret[round_num],\n",
    "            'final_regret_mean': np.mean(final_regrets),\n",
    "            'final_regret_std': np.std(final_regrets)\n",
    "        })\n",
    "\n",
    "print(\"\\nAll simulations completed!\")\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Save results to csv file\n",
    "adversarial_csv_path = data_dir / 'adversarial_fair_payoffs_results.csv'\n",
    "results_df.to_csv(adversarial_csv_path, index=False)\n",
    "print(f\"Results saved to {adversarial_csv_path}\")\n",
    "\n",
    "# Statistical results\n",
    "print(\"\\nFinal regret statistics:\")\n",
    "for epsilon_name in epsilon_values.keys():\n",
    "    subset = results_df[results_df['epsilon_type'] == epsilon_name]\n",
    "    final_stats = subset.iloc[0]  \n",
    "    print(f\"  {epsilon_name}: Mean = {final_stats['final_regret_mean']:.4f}, Std = {final_stats['final_regret_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliPayoffs environment\n",
    "\n",
    "# Save results\n",
    "results_data_bernoulli = []\n",
    "\n",
    "for epsilon_name, epsilon_value in epsilon_values.items():\n",
    "    print(f\"\\nRunning simulations for {epsilon_name} (epsilon = {epsilon_value:.6f})...\")\n",
    "    \n",
    "    regret_histories = []\n",
    "    total_payoffs = []\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        env = BernoulliPayoffs(k)\n",
    "        algorithm = ExponentialWeightsAlgorithm(k, epsilon=epsilon_value, n=n)\n",
    "        \n",
    "        # Run algorithm\n",
    "        regret_history, total_payoff, action_history, cumulative_payoffs = algorithm.run_algorithm(env.generate_payoffs)\n",
    "        \n",
    "        regret_histories.append(regret_history)\n",
    "        total_payoffs.append(total_payoff)\n",
    "        \n",
    "        if (sim + 1) % 20 == 0:\n",
    "            print(f\"  Completed {sim + 1}/{num_simulations} simulations\")\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    regret_array = np.array(regret_histories)\n",
    "    mean_regret = np.mean(regret_array, axis=0)\n",
    "    std_regret = np.std(regret_array, axis=0)\n",
    "    final_regrets = regret_array[:, -1]\n",
    "    \n",
    "    for round_num in range(n):\n",
    "        results_data_bernoulli.append({\n",
    "            'round': round_num + 1,\n",
    "            'epsilon_type': epsilon_name,\n",
    "            'epsilon_value': epsilon_value,\n",
    "            'mean_regret': mean_regret[round_num],\n",
    "            'std_regret': std_regret[round_num],\n",
    "            'final_regret_mean': np.mean(final_regrets),\n",
    "            'final_regret_std': np.std(final_regrets)\n",
    "        })\n",
    "\n",
    "print(\"\\nAll simulations completed!\")\n",
    "\n",
    "results_df_bernoulli = pd.DataFrame(results_data_bernoulli)\n",
    "\n",
    "# Save results to csv file\n",
    "bernoulli_csv_path = data_dir / 'bernoulli_payoffs_results.csv'\n",
    "results_df_bernoulli.to_csv(bernoulli_csv_path, index=False)\n",
    "print(f\"Results saved to {bernoulli_csv_path}\")\n",
    "\n",
    "# Statistical results\n",
    "print(\"\\nFinal regret statistics:\")\n",
    "for epsilon_name in epsilon_values.keys():\n",
    "    subset = results_df_bernoulli[results_df_bernoulli['epsilon_type'] == epsilon_name]\n",
    "    final_stats = subset.iloc[0]  \n",
    "    print(f\"  {epsilon_name}: Mean = {final_stats['final_regret_mean']:.4f}, Std = {final_stats['final_regret_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization functions and run plots\n",
    "import visualization\n",
    "\n",
    "# Load the data\n",
    "adversarial_df = pd.read_csv(adversarial_csv_path)\n",
    "bernoulli_df = pd.read_csv(bernoulli_csv_path)\n",
    "\n",
    "# Use the plot_regret_comparison function from visualization.py\n",
    "visualization.plot_regret_comparison(adversarial_df, 'AdversarialFairPayoffs Environment', 'adversarial_regret_comparison.png')\n",
    "visualization.plot_regret_comparison(bernoulli_df, 'BernoulliPayoffs Environment', 'bernoulli_regret_comparison.png')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
