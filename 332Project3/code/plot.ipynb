{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As project 2, I made function at .py file, and then , plot them in .ipynb file. \n",
    "# I condider Repeated First Price Auction, not second price auction.\n",
    "# Players in Repeated FPA must have algorithm whose regret coverges to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implemented both full and partial information version. \n",
    "# In full information version, I immediately get Nash Equilibrium. a player knows everything including other players' bids and utilities.\n",
    "# In partial information version, a player only knows his bids and his utility, not other players' bids and utilities, which requres need to consider more  complex strategy. \n",
    "\n",
    "# Here is our model(fpa.py).\n",
    "# At each round i, each player decide their bid based on the past information and his/her strategy.\n",
    "# Then, FPA is played.\n",
    "# After FPA, each player get utility.\n",
    "# Then, this process is repeated.\n",
    "\n",
    "# As is often the case, notation is followed.\n",
    "# n is the number of players.\n",
    "# v is the value of the item.\n",
    "# b is the bid of the player.\n",
    "# u is the utility of the player.\n",
    "# p is the probability of winning.\n",
    "\n",
    "# regret was determined by the difference between the utility he could get (best fixed action in hindsight) and the utility he got.\n",
    "\n",
    "# Here is our Assumption.\n",
    "# Partial information\n",
    "# value is determined by the nature, will not change across rounds.\n",
    "# If win, utility is calculated by value - bid, if lose, utility is 0.\n",
    "# If there is a tie in bid, then the players who bid highest get the item by flipping a coin.\n",
    "# discrete value and discrete bid to calculate. \n",
    "\n",
    "# Monte carlo simulation to get the result.\n",
    "# 10000 rounds, 2000 times.\n",
    "\n",
    "# I proposed 7 types of algorithms to consider.\n",
    "# I named them as Myopic Model, Long Model, Optimal Model, Uniform Model, FTL Model, Cool Model, and Feeling Model.\n",
    "# Each algorithm can be defined by a function whose argument is the past information and the number of rounds, and returns the bid.\n",
    "\n",
    "# First, I made myopic algorithm which is tuned to cares about only current round. (1_myopic Model)\n",
    "    # This can be implemented by using our Project1's idea!! culculate other bidder's CDF of bid. \n",
    "    # At each round, it calculate the CDF of other bidders' highest bid, then calculate the probability of winning.\n",
    "    # Hence, we get expected value of bid.\n",
    "\n",
    "# Second, I made algorithm which is tuned to cares about long-term payoff. (2_long Model)\n",
    "    # This is really difficult to implement. \n",
    "    # At each round, it calculate the CDF of other bidders' highest bid.\n",
    "    # I used time-discounted sum, and algorithm will make long-term payoff.\n",
    "    # This algorithm represent the smart player in auction.\n",
    "\n",
    "# Third, I made algorithm which is tuned optimally to balance between two algorithms above. (3_flexible Model)\n",
    "    # This is basic EXponential weight algorithm with nice learning rate = 1/sqrt(n).\n",
    "\n",
    "# Forth, I made algorithm which bid random value between 0 and v. (4_uniform Model)\n",
    "    # I concerned that this algorithm's regret do not converge to 0. \n",
    "\n",
    "# Fifth, I made algorithm which is tuned to be FTL. (5_ftl Model)\n",
    "    # This is basic EXponential weight algorithm with nice learning rate = 100\n",
    "\n",
    "# Sixth, I made algorithm which always bids theoretical optimal value. (6_cool Model)\n",
    "    # Always pay n-1/n * v. \n",
    "\n",
    " # Seventh, I made algorithm which is tuned to choose algorithm based on the past rounds results. (7_feeling Model)\n",
    "    # I model the basic people's feeling of bidding. If you lose many times, you tend to bid higher.\n",
    "    # what is the difference between this and 3_flexible Model is that 6th choose which algorithm to use. \n",
    "    # For example, \n",
    "    # first strategy is to bid aggressively 3/4 of v\n",
    "    # second strategy is to bid normaly 1/2 of v\n",
    "    # third strategy is to bid conservatively 1/4 of v\n",
    "    # if you lose many times, you tend to use first strategy.\n",
    "    # if you win many times, you tend to use second strategy.\n",
    "    # if you win and lose many times, you tend to use third strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect algorithm\n",
    "import  1_myopic\n",
    "import  2_long\n",
    "import  3_flexible\n",
    "import  4_random\n",
    "import  5_cool\n",
    "import  6_feeling\n",
    "import  7_feeling\n",
    "\n",
    "import FPR\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things to show is that every algorithm must have regret converges to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2db87",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simulate the game with mixed players who use above algorithms(This is Part1). \n",
    "\n",
    "# Part 1: Outcomes from No-regret Learning in Games\n",
    "# Experiment design:\n",
    "# - Different combinations of algorithms playing against each other\n",
    "# - For example: Myopic vs Long, Flexible vs FTL, etc.\n",
    "# - Also consider: same algorithm vs different algorithm matchups\n",
    "# - Number of players: n = 3 (or other fixed value)\n",
    "# - Value: v = 10 (or other fixed/discrete values)\n",
    "\n",
    "# Part 1 Questions to Answer:\n",
    "# 1. Do the algorithms converge to a Nash equilibrium?\n",
    "#    - Track bid distributions over time\n",
    "#    - Check if they stabilize to Nash equilibrium bids\n",
    "# 2. In games with multiple Nash equilibria, which one do they converge to?\n",
    "#    - First Price Auction may have multiple equilibria depending on value distributions\n",
    "#    - Need to identify which equilibrium is reached\n",
    "# 3. Convergence rate analysis\n",
    "#    - How many rounds until convergence?\n",
    "\n",
    "# Learning rate analysis:\n",
    "# - For 3_flexible (learning rate = 1/sqrt(n))\n",
    "# - For 5_ftl (learning rate = 100)\n",
    "# - Experiment with different learning rates\n",
    "# - Meta-game: What if players choose learning rates strategically?\n",
    "#   â†’ Nash equilibrium in the meta-game where players choose learning rates\n",
    "\n",
    "# Comparison metrics:\n",
    "# - Average regret over time (should converge to 0)\n",
    "# - Final average utility/payoff\n",
    "# - Convergence speed\n",
    "# - Stability of bids (variance over last N rounds)\n",
    "\n",
    "# Comparison metrics:\n",
    "# - Average regret over time (should converge to 0)\n",
    "# - Final average utility/payoff\n",
    "# - Convergence speed\n",
    "# - Stability of bids (variance over last N rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10620883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf81acd",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part2\n",
    "# - Exploitative strategy vs 1_myopic (Myopic strategy)\n",
    "# - Exploitative strategy vs 2_long (Long-term strategy)\n",
    "# - Exploitative strategy vs 3_flexible (learning rate = 1/sqrt(n))\n",
    "# - Exploitative strategy vs 4_random (Random strategy)\n",
    "# - Exploitative strategy vs 5_ftl (learning rate = 100)\n",
    "# - Exploitative strategy vs 6_cool (Cool strategy)\n",
    "# - Exploitative strategy vs 7_feeling (Feeling strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate of Exploitative strategy for each strategy(1,2,3,4,5,6,7)\n",
    "# Strategy Against 1_myopic : \n",
    "# Strategy Against 2_long : \n",
    "# Strategy Against 3_flexible : \n",
    "# Strategy Against 4_random : \n",
    "# Strategy Against 5_ftl : \n",
    "# Strategy Against 6_cool : \n",
    "# Strategy Against 7_feeling : \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
