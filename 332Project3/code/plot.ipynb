{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2480fd",
   "metadata": {},
   "source": [
    "- As project 2, I made functions at .py file, and then ,plot results in .ipynb file. \n",
    "- I condider Repeated First Price Auction(hereinafter called repeated FPA), not second price auction.\n",
    "- Players in Repeated FPA must have algorithm whose regret coverges to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d25f97",
   "metadata": {},
   "source": [
    "# Project 3: Repeated First Price Auction\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- n_rounds: 1000 - number of rounds per simulation\n",
    "- k: 100 - number of discrete arms (discretization level)\n",
    "- n_mc: 100 - number of Monte Carlo simulation runs\n",
    "- h: scaling parameter (default: value) - used in Exponential Weight algorithms\n",
    "- value (v): 10.0 - player's value for the item (default)\n",
    "- learning_rate: sqrt(log(k) / n) - learning rate for Exponential Weight algorithms (default for flexible)\n",
    "- delta: 0.95 - discount factor for long-term algorithm (default)\n",
    "- observation_rounds: 20 - number of observation rounds for exploitation algorithm (default)\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "1. 1_myopic: Myopic algorithm - maximizes current round expected utility\n",
    "2. 2_long: Long-term algorithm - maximizes discounted long-term utility\n",
    "3. 3_flexible: Flexible algorithm - Exponential Weight with learning_rate = np.sqrt(np.log(k) / n)\n",
    "4. 4_cool: Cool algorithm - always bids v/2\n",
    "5. 5_feeling: Feeling algorithm - Exponential Weight over 5 strategy arms\n",
    "6. 6_Exploitation: Exploitation algorithm - waits and exploits when opponent bids low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0c2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implemented both full and partial information version. \n",
    "# In full information version, I immediately get Nash Equilibrium. a player knows everything including other players' bids and utilities.\n",
    "# In partial information version, a player only knows his bids and his utility, not other players' bids and utilities, which requres need to consider more  complex strategy. \n",
    "\n",
    "# Here is our model(fpa.py).\n",
    "# At each round i, each player decide their bid based on the past information and his/her strategy.\n",
    "# Then, FPA is played.\n",
    "# After FPA, each player get utility.\n",
    "# Then, this process is repeated.\n",
    "\n",
    "# As is often the case, notation is followed.\n",
    "# n is the number of players.\n",
    "# v is the value of the item.\n",
    "# b is the bid of the player.\n",
    "# u is the utility of the player.\n",
    "# p is the probability of winning.\n",
    "\n",
    "# regret was determined by the difference between the utility he could get (best fixed action in hindsight) and the utility he got.\n",
    "\n",
    "# Here is our Assumption.\n",
    "# Partial information\n",
    "# value is determined by the nature, will not change across rounds.\n",
    "# If win, utility is calculated by value - bid, if lose, utility is 0.\n",
    "# If there is a tie in bid, then the players who bid highest get the item by flipping a coin.\n",
    "# discrete value and discrete bid to calculate. \n",
    "\n",
    "# Monte carlo simulation to get the result.\n",
    "# 10000 rounds, 2000 times.\n",
    "\n",
    "# I proposed 7 types of algorithms to consider.\n",
    "# I named them as Myopic Model, Long Model, Optimal Model, Uniform Model, FTL Model, Cool Model, and Feeling Model.\n",
    "# Each algorithm can be defined by a function whose argument is the past information and the number of rounds, and returns the bid.\n",
    "\n",
    "# First, I made myopic algorithm which is tuned to cares about only current round. (1_myopic Model)\n",
    "    # This can be implemented by using our Project1's idea!! culculate other bidder's CDF of bid. \n",
    "    # At each round, it calculate the CDF of other bidders' highest bid, then calculate the probability of winning.\n",
    "    # Hence, we get expected value of bid.\n",
    "\n",
    "# Second, I made algorithm which is tuned to cares about long-term payoff. (2_long Model)\n",
    "    # This is really difficult to implement. \n",
    "    # At each round, it calculate the CDF of other bidders' highest bid.\n",
    "    # I used time-discounted sum, and algorithm will make long-term payoff.\n",
    "    # This algorithm represent the smart player in auction.\n",
    "\n",
    "# Third, I made algorithm which is tuned optimally to balance between two algorithms above. (3_flexible Model)\n",
    "    # This is basic EXponential weight algorithm with nice learning rate = sqrt(log(k) / n).\n",
    "\n",
    "# Forth, I made algorithm which bid random value between 0 and v. (4_uniform Model)\n",
    "    # I concerned that this algorithm's regret do not converge to 0. \n",
    "\n",
    "# Fifth, I made algorithm which is tuned to be FTL. (5_ftl Model)\n",
    "    # This is basic EXponential weight algorithm with nice learning rate = 100\n",
    "\n",
    "# Sixth, I made algorithm which always bids theoretical optimal value. (4_cool Model)\n",
    "    # Always pay n-1/n * v. \n",
    "\n",
    " # Seventh, I made algorithm which is tuned to choose algorithm based on the past rounds results. (5_feeling Model)\n",
    "    # I model the basic people's feeling of bidding. If you lose many times, you tend to bid higher.\n",
    "    # what is the difference between this and 3_flexible Model is that 6th choose which algorithm to use. \n",
    "    # For example, \n",
    "    # first strategy is to bid aggressively 3/4 of v\n",
    "    # second strategy is to bid normaly 1/2 of v\n",
    "    # third strategy is to bid conservatively 1/4 of v\n",
    "    # if you lose many times, you tend to use first strategy.\n",
    "    # if you win many times, you tend to use second strategy.\n",
    "    # if you win and lose many times, you tend to use third strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect algorithm\n",
    "import importlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add algorithm directory to path (works in Jupyter notebooks)\n",
    "algorithm_dir = Path('algorithm')\n",
    "if str(algorithm_dir.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(algorithm_dir.resolve()))\n",
    "\n",
    "# Import modules with numeric names using importlib and create aliases\n",
    "myopic = importlib.import_module('1_myopic')\n",
    "long = importlib.import_module('2_long')\n",
    "flexible = importlib.import_module('3_flexible')\n",
    "# Note: 4_random and 5_ftl are removed - they can be represented by flexible with different learning rates\n",
    "# 4_random: random strategy (can be removed as it doesn't converge to 0 regret)\n",
    "# 5_ftl: flexible with learning_rate = 100\n",
    "cool = importlib.import_module('4_cool')\n",
    "feeling = importlib.import_module('5_feeling')\n",
    "exploitation = importlib.import_module('6_Exploitation')\n",
    "\n",
    "# Import other modules\n",
    "import two_repeatedFPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2db87",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simulate the game with mixed players who use above algorithms(This is Part1). \n",
    "\n",
    "# Part 1: Outcomes from No-regret Learning in Games\n",
    "# Experiment design:\n",
    "# - Different combinations of algorithms playing against each other\n",
    "# - For example: Myopic vs Long, Flexible vs FTL, etc.\n",
    "# - Also consider: same algorithm vs different algorithm matchups\n",
    "# - Number of players: n = 3 (or other fixed value)\n",
    "# - Value: v = 10 (or other fixed/discrete values)\n",
    "\n",
    "# Part 1 Questions to Answer:\n",
    "# 1. Do the algorithms converge to a Nash equilibrium?\n",
    "#    - Track bid distributions over time\n",
    "#    - Check if they stabilize to Nash equilibrium bids\n",
    "# 2. In games with multiple Nash equilibria, which one do they converge to?\n",
    "#    - First Price Auction may have multiple equilibria depending on value distributions\n",
    "#    - Need to identify which equilibrium is reached\n",
    "# 3. Convergence rate analysis\n",
    "#    - How many rounds until convergence?\n",
    "\n",
    "# Learning rate analysis:\n",
    "# - For 3_flexible (learning rate = sqrt(log(k) / n))\n",
    "# - For 5_ftl (learning rate = 100)\n",
    "# - Experiment with different learning rates\n",
    "# - Meta-game: What if players choose learning rates strategically?\n",
    "#   → Nash equilibrium in the meta-game where players choose learning rates\n",
    "\n",
    "# Comparison metrics:\n",
    "# - Average regret over time (should converge to 0)\n",
    "# - Final average utility/payoff\n",
    "# - Convergence speed\n",
    "# - Stability of bids (variance over last N rounds)\n",
    "\n",
    "# Comparison metrics:\n",
    "# - Average regret over time (should converge to 0)\n",
    "# - Final average utility/payoff\n",
    "# - Convergence speed\n",
    "# - Stability of bids (variance over last N rounds)\n",
    "\n",
    "\n",
    "# まず, EX3のために ,b round, k arm 離散化の度合い, h : scaling parameterを設定したい\n",
    "# n = 1000, \n",
    "# k = 100\n",
    "# h は各々のvalue.\n",
    "\n",
    "#　regret, utility, 勝率の変化を毎回のMCの際に記録しておく必要がある."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c7377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 simulation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Part 1 Implementation\n",
    "# Parameters\n",
    "n_rounds = 1000  # number of rounds\n",
    "k = 100  # number of arms (discretization)\n",
    "n_mc = 100  # number of Monte Carlo simulations\n",
    "\n",
    "# Import simulation and plotting functions from two_repeatedFPA.py\n",
    "from two_repeatedFPA import run_repeated_fpa, plot_part1_results\n",
    "\n",
    "print(\"Part 1 simulation functions imported from two_repeatedFPA.py\")\n",
    "    \"\"\"\n",
    "    Run repeated First Price Auction simulation\n",
    "    \n",
    "    Args:\n",
    "        player1_config: tuple of (algorithm_function, value, env_state_dict)\n",
    "        player2_config: tuple of (algorithm_function, value, env_state_dict)\n",
    "        n_rounds: number of rounds per simulation\n",
    "        n_mc: number of Monte Carlo runs\n",
    "    \n",
    "    Returns:\n",
    "        results: dict with regret, utility, win_rate for each player\n",
    "    \"\"\"\n",
    "    alg1_func, v1, env1 = player1_config\n",
    "    alg2_func, v2, env2 = player2_config\n",
    "    \n",
    "    # Initialize environment state for each player\n",
    "    env1.setdefault('k', k)\n",
    "    env1.setdefault('h', v1)\n",
    "    env2.setdefault('k', k)\n",
    "    env2.setdefault('h', v2)\n",
    "    \n",
    "    # Storage for all MC runs\n",
    "    all_regret1 = []\n",
    "    all_regret2 = []\n",
    "    all_utility1 = []\n",
    "    all_utility2 = []\n",
    "    all_win_rate1 = []\n",
    "    all_win_rate2 = []\n",
    "    all_bids1 = []\n",
    "    all_bids2 = []\n",
    "    all_regret_history1 = []\n",
    "    all_regret_history2 = []\n",
    "    \n",
    "    # Monte Carlo loop\n",
    "    for mc_iter in range(n_mc):\n",
    "        # Reset for each MC run\n",
    "        history1 = []\n",
    "        history2 = []\n",
    "        env1_state = env1.copy()\n",
    "        env2_state = env2.copy()\n",
    "        env1_state['cumulative_payoffs'] = np.zeros(k)\n",
    "        env2_state['cumulative_payoffs'] = np.zeros(k)\n",
    "        \n",
    "        total_utility1 = 0\n",
    "        total_utility2 = 0\n",
    "        wins1 = 0\n",
    "        wins2 = 0\n",
    "        \n",
    "        bids1_history = []\n",
    "        bids2_history = []\n",
    "        regret_history1 = []\n",
    "        regret_history2 = []\n",
    "        opponent_bids1 = []\n",
    "        opponent_bids2 = []\n",
    "        \n",
    "        # Round loop\n",
    "        for round_num in range(n_rounds):\n",
    "            # Get bids from each player\n",
    "            bid1 = alg1_func(0, v1, round_num, history1, env1_state)\n",
    "            bid2 = alg2_func(1, v2, round_num, history2, env2_state)\n",
    "            \n",
    "            bids1_history.append(bid1)\n",
    "            bids2_history.append(bid2)\n",
    "            \n",
    "            # Play FPA\n",
    "            auction = fpa.FPA()\n",
    "            alloc1, alloc2 = auction.play_round(bid1, bid2)\n",
    "            \n",
    "            # Calculate utilities\n",
    "            utility1 = utility.calculate_utility(v1, alloc1, bid1)\n",
    "            utility2 = utility.calculate_utility(v2, alloc2, bid2)\n",
    "            \n",
    "            # Update totals\n",
    "            total_utility1 += utility1\n",
    "            total_utility2 += utility2\n",
    "            \n",
    "            # Track wins\n",
    "            if alloc1 > 0.5:\n",
    "                wins1 += 1\n",
    "            elif alloc2 > 0.5:\n",
    "                wins2 += 1\n",
    "            else:  # tie\n",
    "                if np.random.random() < 0.5:\n",
    "                    wins1 += 1\n",
    "                else:\n",
    "                    wins2 += 1\n",
    "            \n",
    "            # Update history\n",
    "            won1 = (alloc1 > 0.5) or (alloc1 == 0.5 and np.random.random() < 0.5)\n",
    "            won2 = (alloc2 > 0.5) or (alloc2 == 0.5 and np.random.random() < 0.5)\n",
    "            history1.append((bid1, utility1, won1))\n",
    "            history2.append((bid2, utility2, won2))\n",
    "            \n",
    "            # Store opponent bids for regret calculation\n",
    "            opponent_bids1.append(bid2)\n",
    "            opponent_bids2.append(bid1)\n",
    "            \n",
    "            # Calculate regret: best fixed action in hindsight\n",
    "            bid_grid = np.linspace(0, max(v1, v2), k)\n",
    "            \n",
    "            best_fixed_utility1 = 0\n",
    "            for fixed_bid in bid_grid:\n",
    "                if fixed_bid > v1:\n",
    "                    continue\n",
    "                fixed_utility1 = sum([\n",
    "                    utility.calculate_utility(v1,\n",
    "                        fpa.FPA().play_round(fixed_bid, opp_bid)[0],\n",
    "                        fixed_bid)\n",
    "                    for opp_bid in opponent_bids1\n",
    "                ])\n",
    "                best_fixed_utility1 = max(best_fixed_utility1, fixed_utility1)\n",
    "            \n",
    "            best_fixed_utility2 = 0\n",
    "            for fixed_bid in bid_grid:\n",
    "                if fixed_bid > v2:\n",
    "                    continue\n",
    "                fixed_utility2 = sum([\n",
    "                    utility.calculate_utility(v2,\n",
    "                        fpa.FPA().play_round(opp_bid, fixed_bid)[1],\n",
    "                        fixed_bid)\n",
    "                    for opp_bid in opponent_bids2\n",
    "                ])\n",
    "                best_fixed_utility2 = max(best_fixed_utility2, fixed_utility2)\n",
    "            \n",
    "            # Calculate regret at this round\n",
    "            regret1_round = best_fixed_utility1 - total_utility1\n",
    "            regret2_round = best_fixed_utility2 - total_utility2\n",
    "            regret_history1.append(regret1_round)\n",
    "            regret_history2.append(regret2_round)\n",
    "        \n",
    "        all_regret1.append(regret_history1[-1])\n",
    "        all_regret2.append(regret_history2[-1])\n",
    "        all_utility1.append(total_utility1)\n",
    "        all_utility2.append(total_utility2)\n",
    "        all_win_rate1.append(wins1 / n_rounds)\n",
    "        all_win_rate2.append(wins2 / n_rounds)\n",
    "        all_bids1.append(bids1_history)\n",
    "        all_bids2.append(bids2_history)\n",
    "        all_regret_history1.append(regret_history1)\n",
    "        all_regret_history2.append(regret_history2)\n",
    "        \n",
    "        if (mc_iter + 1) % 10 == 0:\n",
    "            print(f\"MC iteration {mc_iter + 1}/{n_mc} completed\")\n",
    "    \n",
    "    return {\n",
    "        'regret1': np.array(all_regret1),\n",
    "        'regret2': np.array(all_regret2),\n",
    "        'utility1': np.array(all_utility1),\n",
    "        'utility2': np.array(all_utility2),\n",
    "        'win_rate1': np.array(all_win_rate1),\n",
    "        'win_rate2': np.array(all_win_rate2),\n",
    "        'bids1': all_bids1,\n",
    "        'bids2': all_bids2,\n",
    "        'regret_history1': all_regret_history1,\n",
    "        'regret_history2': all_regret_history2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting functions defined\n"
     ]
    }
   ],
   "source": [
    "# Plotting functions are now in two_repeatedFPA.py\n",
    "# Use plot_part1_results() from two_repeatedFPA module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Myopic vs Flexible\n",
      "MC iteration 10/100 completed\n",
      "MC iteration 20/100 completed\n",
      "MC iteration 30/100 completed\n"
     ]
    }
   ],
   "source": [
    "# Run simulation: Myopic vs Flexible\n",
    "print(\"Running: Myopic vs Flexible\")\n",
    "v1, v2 = 10.0, 10.0\n",
    "player1 = (myopic.myopic_algorithm, v1, {'k': k, 'h': v1})\n",
    "player2 = (flexible.flexible_algorithm, v2, {'k': k, 'h': v2, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "results_myopic_vs_flexible = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "print(\"Completed: Myopic vs Flexible\")\n",
    "plot_part1_results(results_myopic_vs_flexible, title=\"Myopic vs Flexible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run simulation with different algorithm combinations\n",
    "# Test different matchups\n",
    "\n",
    "# Note: n_mc = 100 can take a long time. For quick testing, use n_mc = 10\n",
    "# Uncomment the examples below to run simulations\n",
    "\n",
    "# Example 1: Myopic vs Flexible\n",
    "# print(\"Running: Myopic vs Flexible\")\n",
    "# v1, v2 = 10.0, 10.0\n",
    "# player1 = (myopic.myopic_algorithm, v1, {'k': k, 'h': v1})\n",
    "# player2 = (flexible.flexible_algorithm, v2, {'k': k, 'h': v2, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "# results_myopic_vs_flexible = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "# print(\"Completed: Myopic vs Flexible\")\n",
    "# plot_part1_results(results_myopic_vs_flexible, title=\"Myopic vs Flexible\")\n",
    "\n",
    "# Example 2: Flexible vs Exploitation\n",
    "# print(\"Running: Flexible vs Exploitation\")\n",
    "# v1, v2 = 10.0, 10.0\n",
    "# player1 = (flexible.flexible_algorithm, v1, {'k': k, 'h': v1, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "# player2 = (exploitation.exploitation_algorithm, v2, {'k': k, 'h': v2, 'observation_rounds': 20})\n",
    "# results_flexible_vs_exploitation = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "# print(\"Completed: Flexible vs Exploitation\")\n",
    "# plot_part1_results(results_flexible_vs_exploitation, title=\"Flexible vs Exploitation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf81acd",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part2\n",
    "# - Exploitative strategy vs 1_myopic (Myopic strategy)\n",
    "# - Exploitative strategy vs 2_long (Long-term strategy)\n",
    "# - Exploitative strategy vs 3_flexible (learning rate = sqrt(log(k) / n))\n",
    "# - Exploitative strategy vs 4_random (Random strategy)\n",
    "# - Exploitative strategy vs 5_ftl (learning rate = 100)\n",
    "# - Exploitative strategy vs 4_cool (Cool strategy)\n",
    "# - Exploitative strategy vs 5_feeling (Feeling strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate of Exploitative strategy for each strategy(1,2,3,4,5,6,7)\n",
    "# Strategy Against 1_myopic : \n",
    "# Strategy Against 2_long : \n",
    "# Strategy Against 3_flexible : \n",
    "# Strategy Against 4_random : \n",
    "# Strategy Against 5_ftl : \n",
    "# Strategy Against 4_cool : \n",
    "# Strategy Against 5_feeling : "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
