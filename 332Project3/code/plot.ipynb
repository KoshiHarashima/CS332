{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2480fd",
   "metadata": {},
   "source": [
    "- As project 2, I made functions at .py file, and then ,plot results in .ipynb file. \n",
    "- I condider Repeated First Price Auction(hereinafter called repeated FPA), not second price auction.\n",
    "- Players in Repeated FPA must have algorithm whose regret coverges to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d25f97",
   "metadata": {},
   "source": [
    "# Project 3: Repeated First Price Auction\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- n_rounds: 1000 - number of rounds per simulation\n",
    "- k: 100 - number of discrete arms (discretization level)\n",
    "- n_mc: 100 - number of Monte Carlo simulation runs\n",
    "- h: scaling parameter (default: value) - used in Exponential Weight algorithms\n",
    "- value (v): 10.0 - player's value for the item (default)\n",
    "- learning_rate: sqrt(log(k) / n) - learning rate for Exponential Weight algorithms (default for flexible)\n",
    "- delta: 0.95 - discount factor for long-term algorithm (default)\n",
    "- observation_rounds: 20 - number of observation rounds for exploitation algorithm (default)\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "1. 1_myopic: Myopic algorithm - maximizes current round expected utility\n",
    "2. 2_long: Long-term algorithm - maximizes discounted long-term utility\n",
    "3. 3_flexible: Flexible algorithm - Exponential Weight with learning_rate = np.sqrt(np.log(k) / n)\n",
    "4. 4_cool: Cool algorithm - always bids v/2\n",
    "5. 5_feeling: Feeling algorithm - Exponential Weight over 5 strategy arms\n",
    "6. 6_Exploitation: Exploitation algorithm - waits and exploits when opponent bids low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0c2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implemented both full and partial information version. \n",
    "# In full information version, I immediately get Nash Equilibrium. a player knows everything including other players' bids and utilities.\n",
    "# In partial information version, a player only knows his bids and his utility, not other players' bids and utilities, which requres need to consider more  complex strategy. \n",
    "\n",
    "# Here is our model(fpa.py).\n",
    "# At each round i, each player decide their bid based on the past information and his/her strategy.\n",
    "# Then, FPA is played.\n",
    "# After FPA, each player get utility.\n",
    "# Then, this process is repeated.\n",
    "\n",
    "# As is often the case, notation is followed.\n",
    "# n is the number of players.\n",
    "# v is the value of the item.\n",
    "# b is the bid of the player.\n",
    "# u is the utility of the player.\n",
    "# p is the probability of winning.\n",
    "\n",
    "# regret was determined by the difference between the utility he could get (best fixed action in hindsight) and the utility he got.\n",
    "\n",
    "# Here is our Assumption.\n",
    "# Partial information\n",
    "# value is determined by the nature, will not change across rounds.\n",
    "# If win, utility is calculated by value - bid, if lose, utility is 0.\n",
    "# If there is a tie in bid, then the players who bid highest get the item by flipping a coin.\n",
    "# discrete value and discrete bid to calculate. \n",
    "\n",
    "# Monte carlo simulation to get the result.\n",
    "# 10000 rounds, 2000 times.\n",
    "\n",
    "# I proposed 7 types of algorithms to consider.\n",
    "# I named them as Myopic Model, Long Model, Optimal Model, Uniform Model, FTL Model, Cool Model, and Feeling Model.\n",
    "# Each algorithm can be defined by a function whose argument is the past information and the number of rounds, and returns the bid.\n",
    "\n",
    "# First, I made myopic algorithm which is tuned to cares about only current round. (1_myopic Model)\n",
    "    # This can be implemented by using our Project1's idea!! culculate other bidder's CDF of bid. \n",
    "    # At each round, it calculate the CDF of other bidders' highest bid, then calculate the probability of winning.\n",
    "    # Hence, we get expected value of bid.\n",
    "\n",
    "# Second, I made algorithm which is tuned to cares about long-term payoff. (2_long Model)\n",
    "    # This is really difficult to implement. \n",
    "    # At each round, it calculate the CDF of other bidders' highest bid.\n",
    "    # I used time-discounted sum, and algorithm will make long-term payoff.\n",
    "    # This algorithm represent the smart player in auction.\n",
    "\n",
    "# Third, I made algorithm which is tuned optimally to balance between two algorithms above. (3_flexible Model)\n",
    "    # This is basic EXponential weight algorithm with nice learning rate = sqrt(log(k) / n).\n",
    "\n",
    "# Forth, I made algorithm which bid random value between 0 and v. (4_uniform Model)\n",
    "    # I concerned that this algorithm's regret do not converge to 0. \n",
    "\n",
    "# Fifth, I made algorithm which is tuned to be FTL. (5_ftl Model)\n",
    "    # This is basic EXponential weight algorithm with nice learning rate = 100\n",
    "\n",
    "# Sixth, I made algorithm which always bids theoretical optimal value. (4_cool Model)\n",
    "    # Always pay n-1/n * v. \n",
    "\n",
    " # Seventh, I made algorithm which is tuned to choose algorithm based on the past rounds results. (5_feeling Model)\n",
    "    # I model the basic people's feeling of bidding. If you lose many times, you tend to bid higher.\n",
    "    # what is the difference between this and 3_flexible Model is that 6th choose which algorithm to use. \n",
    "    # For example, \n",
    "    # first strategy is to bid aggressively 3/4 of v\n",
    "    # second strategy is to bid normaly 1/2 of v\n",
    "    # third strategy is to bid conservatively 1/4 of v\n",
    "    # if you lose many times, you tend to use first strategy.\n",
    "    # if you win many times, you tend to use second strategy.\n",
    "    # if you win and lose many times, you tend to use third strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect algorithm\n",
    "import importlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add algorithm directory to path (works in Jupyter notebooks)\n",
    "algorithm_dir = Path('algorithm')\n",
    "if str(algorithm_dir.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(algorithm_dir.resolve()))\n",
    "\n",
    "# Import modules with numeric names using importlib and create aliases\n",
    "myopic = importlib.import_module('1_myopic')\n",
    "long = importlib.import_module('2_long')\n",
    "flexible = importlib.import_module('3_flexible')\n",
    "# Note: 4_random and 5_ftl are removed - they can be represented by flexible with different learning rates\n",
    "# 4_random: random strategy (can be removed as it doesn't converge to 0 regret)\n",
    "# 5_ftl: flexible with learning_rate = 100\n",
    "cool = importlib.import_module('4_cool')\n",
    "feeling = importlib.import_module('5_feeling')\n",
    "exploitation = importlib.import_module('6_Exploitation')\n",
    "\n",
    "# Import other modules\n",
    "import two_repeatedFPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2db87",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "- we simulate the game with mixed players who use above algorithms(This is Part1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c7377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 simulation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Part 1 Implementation\n",
    "# Parameters\n",
    "n_rounds = 1000  # number of rounds\n",
    "k = 100  # number of arms (discretization)\n",
    "n_mc = 100  # number of Monte Carlo simulations\n",
    "\n",
    "# Import simulation and plotting functions from two_repeatedFPA.py\n",
    "from two_repeatedFPA import run_repeated_fpa, plot_part1_results\n",
    "\n",
    "print(\"Part 1 simulation functions imported from two_repeatedFPA.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting functions defined\n"
     ]
    }
   ],
   "source": [
    "# Plotting functions are now in two_repeatedFPA.py\n",
    "# Use plot_part1_results() from two_repeatedFPA module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Myopic vs Flexible\n",
      "MC iteration 10/100 completed\n",
      "MC iteration 20/100 completed\n",
      "MC iteration 30/100 completed\n",
      "MC iteration 40/100 completed\n",
      "MC iteration 50/100 completed\n"
     ]
    }
   ],
   "source": [
    "# Run simulation: Myopic vs Flexible\n",
    "print(\"Running: Myopic vs Flexible\")\n",
    "v1, v2 = 10.0, 10.0\n",
    "player1 = (myopic.myopic_algorithm, v1, {'k': k, 'h': v1})\n",
    "player2 = (flexible.flexible_algorithm, v2, {'k': k, 'h': v2, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "results_myopic_vs_flexible = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "print(\"Completed: Myopic vs Flexible\")\n",
    "plot_part1_results(results_myopic_vs_flexible, title=\"Myopic vs Flexible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run simulation with different algorithm combinations\n",
    "# Test different matchups\n",
    "\n",
    "# Note: n_mc = 100 can take a long time. For quick testing, use n_mc = 10\n",
    "# Uncomment the examples below to run simulations\n",
    "\n",
    "# Example 1: Myopic vs Flexible\n",
    "# print(\"Running: Myopic vs Flexible\")\n",
    "# v1, v2 = 10.0, 10.0\n",
    "# player1 = (myopic.myopic_algorithm, v1, {'k': k, 'h': v1})\n",
    "# player2 = (flexible.flexible_algorithm, v2, {'k': k, 'h': v2, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "# results_myopic_vs_flexible = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "# print(\"Completed: Myopic vs Flexible\")\n",
    "# plot_part1_results(results_myopic_vs_flexible, title=\"Myopic vs Flexible\")\n",
    "\n",
    "# Example 2: Flexible vs Exploitation\n",
    "# print(\"Running: Flexible vs Exploitation\")\n",
    "# v1, v2 = 10.0, 10.0\n",
    "# player1 = (flexible.flexible_algorithm, v1, {'k': k, 'h': v1, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "# player2 = (exploitation.exploitation_algorithm, v2, {'k': k, 'h': v2, 'observation_rounds': 20})\n",
    "# results_flexible_vs_exploitation = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "# print(\"Completed: Flexible vs Exploitation\")\n",
    "# plot_part1_results(results_flexible_vs_exploitation, title=\"Flexible vs Exploitation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf81acd",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part2\n",
    "# - Exploitative strategy vs 1_myopic (Myopic strategy)\n",
    "# - Exploitative strategy vs 2_long (Long-term strategy)\n",
    "# - Exploitative strategy vs 3_flexible (learning rate = sqrt(log(k) / n))\n",
    "# - Exploitative strategy vs 4_random (Random strategy)\n",
    "# - Exploitative strategy vs 5_ftl (learning rate = 100)\n",
    "# - Exploitative strategy vs 4_cool (Cool strategy)\n",
    "# - Exploitative strategy vs 5_feeling (Feeling strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b907e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate of Exploitative strategy for each strategy(1,2,3,4,5,6,7)\n",
    "# Strategy Against 1_myopic : \n",
    "# Strategy Against 2_long : \n",
    "# Strategy Against 3_flexible : \n",
    "# Strategy Against 4_cool : \n",
    "# Strategy Against 5_feeling : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Flexible vs Exploitation\n",
    "print(\"Running: Flexible vs Exploitation (Part 2)\")\n",
    "v1, v2 = 10.0, 10.0\n",
    "player1 = (flexible.flexible_algorithm, v1, {'k': k, 'h': v1, 'learning_rate': np.sqrt(np.log(k) / n_rounds)})\n",
    "player2 = (exploitation.exploitation_algorithm, v2, {'k': k, 'h': v2, 'observation_rounds': 20})\n",
    "results_flexible_vs_exploitation = run_repeated_fpa(player1, player2, n_rounds, n_mc, k=k)\n",
    "print(\"Completed: Flexible vs Exploitation\")\n",
    "plot_part1_results(results_flexible_vs_exploitation, title=\"Flexible vs Exploitation (Part 2)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
