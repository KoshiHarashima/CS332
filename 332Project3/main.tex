\documentclass[10pt]{article}
\usepackage[margin=0.55in,top=0.5in,bottom=0.5in]{geometry}
\usepackage[all]{xy}
\usepackage{caption}
\usepackage{amsmath,amsthm,amssymb,color,latexsym}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{letterpaper}    
\usepackage{graphicx}
\usepackage{tabto}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}

\usepackage[noframe]{showframe}
\usepackage{framed}


\renewenvironment{shaded}{%
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{shadecolor}}%
  \MakeFramed{\advance\hsize-\width \FrameRestore\FrameRestore}}%
 {\endMakeFramed}
\definecolor{shadecolor}{gray}{0.90}

\newtheorem{problem}{Problem}

\newenvironment{solution}[1][\it{Solution:}]{\textbf{#1 } }

\singlespacing

\begin{document}
\graphicspath{{../figures/}{figures/}{332Project2/figures/}}

\begin{titlepage}
   \begin{center}
       \vspace*{9cm}

       \textbf{CS 332 Fall 2025}

       \vspace{0.5cm}
        Project \#3
        \vfill

       \textbf{Koshi Harashima, Ben Cole}\\
       Due Date: 11/5, 2025
            
   \end{center}
\end{titlepage}

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% Problems %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------------------
\section{Outcomes from No-regret Learning in Games}
\begin{shaded}
\textbf{Methods}\\
Repeated first-price auction (2 bidders, fixed values, full information + full feedback). Utility if win $u=v-b$, else $0$; ties split $0.5$.\\
\textit{Algorithms}: Myopic (one-step expected utility using empirical $P(\mathrm{win}\mid b)$) vs Flexible (Exponential Weights/Hedge over $k$ bids with full-feedback updates, $\epsilon=\sqrt{\log k/n}$).\\
Setup: $(v_1,v_2) \in \{(10,10),(12,8)\}$, $k\in\{10,100\}$, rounds $n\in\{100,1000\}$, Monte Carlo $n_{mc}=10$. NE proxies computed at $(12,8)$ with $k{=}100$, $n{=}1000$ and a longer run $n{=}5000$.

\textbf{Results}\\
Myopic vs Flexible (10,10; $k{=}10$; $n{=}1000$; $n_{mc}{=}10$): Myopic mean utility $\approx 1022.8$, win rate $\approx 0.426$; Flexible $\approx 1027.7$, $\approx 0.574$. Regret vs best fixed bid is not uniformly lower for Flexible with coarse $k{=}10$; Flexible bids are more stable (lower variance). Robustness across $n,k,$ values preserves the ranking (Flexible $>$ Myopic); scaling $\epsilon$ in $\{0.5,1,2\}\times$ default changes variance, not ranking.\\
NE proxies (no pure NE for complete-info FPA with $v_1>v_2$; mixed equilibrium predicts efficient allocation and price near $v_L$): $(12,8)$, $k{=}100$: efficiency $0.946$ (all rounds, $n{=}1000$) $\to$ $0.959$ (last 500 of $n{=}5000$); average winning price $7.80 \to 7.89$ vs $v_L{=}8.0$.

\textbf{Takeaways}\\
Flexible (Hedge) consistently achieves higher average utility and win rate than Myopic; Myopic overreacts to recent bids. Outcomes move toward mixed-equilibrium predictions (high-value wins; price near $v_L$) as $n$ (and $k$) increase, but do not converge to an exact equilibrium strategy profile.
\end{shaded}

\section{Manipulability of No-regret Learners in Games}
\begin{shaded}
\textbf{Model}\\
Same repeated FPA setting. Opponent: Flexible learner with $v_1{=}9$, $k{=}100$. Our player (exploiter) has $v_2{=}3$ and: observes for a few rounds (bid $\approx 0.2v$); predicts the opponent’s next bid via exponential smoothing ($\alpha\in\{0.2,0.4,0.6\}$); when predicted bid falls below a threshold ratio ($\in\{0.5,0.6,0.7\}$), bids to win with positive margin on the grid. Main run $n{=}100$, $n_{mc}{=}1$; robustness with $n_{mc}{=}5$.
Full feedback enables computing $P(\mathrm{win}\mid b)$ for all grid bids and timing the exploit when the flexible learner’s sampling puts mass on low bids.

\textbf{Results}\\
Baseline (obs$\,{=}\,5$): Flexible win rate $\approx 0.91$, mean utility $\approx 618$; exploiter win rate $\approx 0.09$, mean utility $\approx 11$. Robustness (obs $\in\{5,10,15\}$, $\alpha\in\{0.2,0.4,0.6\}$, threshold $\in\{0.5,0.6,0.7\}$; $n_{mc}{=}5$): exploiter win rate $\approx 0.08$–$0.10$, mean utility $\approx 9$–$12$; Flexible remains $\approx 0.90$–$0.92$ wins with high utility.

\textbf{Takeaways}\\
Under full feedback, prediction-based exploitation yields occasional low-price wins for a low-value bidder, but gains are small; the flexible learner remains dominant. The effect is robust to reasonable observation/smoothing/threshold choices.
\end{shaded}

\section*{AI and Collaboration}
\begin{shaded}
Both authors jointly designed experiments and interpreted results;\\
AI was used to assist with code organization, plotting, slide layout, and editing text; all code and results were reviewed and validated by the authors.
\end{shaded}

\section*{Figures}
\vspace{-0.3em}
\begin{figure}[h!]
\centering
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{332Project3/figures/empirical_vs_ew_bid_evolution.png}
    \caption*{\footnotesize }
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{332Project3/figures/empirical_vs_ew_bid_evolution.png}
    \caption*{\footnotesize }
\end{minipage}
\end{figure}

\end{document}