\documentclass[11pt]{article}
\usepackage[margin=.75in]{geometry}
\usepackage[all]{xy}

\usepackage{amsmath,amsthm,amssymb,color,latexsym}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{letterpaper}    
\usepackage{graphicx}
\usepackage{tabto}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}

\usepackage[noframe]{showframe}
\usepackage{framed}

\renewenvironment{shaded}{%
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{shadecolor}}%
  \MakeFramed{\advance\hsize-\width \FrameRestore\FrameRestore}}%
 {\endMakeFramed}
\definecolor{shadecolor}{gray}{0.90}

\newtheorem{problem}{Problem}

\newenvironment{solution}[1][\it{Solution:}]{\textbf{#1 } }

\onehalfspacing

\begin{document}
\begin{titlepage}
   \begin{center}
       \vspace*{9cm}

       \textbf{CS 332 Fall 2025}

       \vspace{0.5cm}
        Problem Set \#3
        \vfill

       \textbf{Koshi Harashima}\\
       Due Date: 10/17, 2025
            
   \end{center}
\end{titlepage}

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% Problems %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------------------
\begin{shaded}
\section*{Problem 1: Learning to maximize two-dimensional functions}
\textbf{Conclusion.} Running EXP3 on a $k\times k$ grid with $k \asymp (n/\log n)^{1/4}$ gives average regret $R_n=\tilde{O}(n^{-1/4})$.

\paragraph{Algorithm.}
Partition $[0,1]^2$ into $k\times k$ equal cells and let $\mathcal{A}$ be the $K=k^2$ cell centers.
Initialize $w_1(a)=1$ for all $a\in\mathcal{A}$. For rounds $i=1,\dots,n$:
\begin{enumerate}
\item Form a sampling distribution 
\[
p_i(a)=(1-\gamma)\,\frac{w_i(a)}{\sum_{b}w_i(b)}+\gamma/K
\]
\item Draw $a_i\sim p_i$, play its center $x_i\in[0,1]^2$, observe reward $r_i=f_i(x_i)\in[0,1]$.
\item Set the importance-weighted estimate $\hat r_i(a)=\frac{r_i}{p_i(a)}\mathbf{1}\{a=a_i\}$.
\item Update $w_{i+1}(a)=w_i(a)\exp(\eta\,\hat r_i(a))$.
\end{enumerate}
Take $\gamma \asymp \sqrt{(K\log K)/n}$ and $\eta=\gamma/K$.

\paragraph{Why it works (short).}
\begin{itemize}
\item \emph{Lipschitz from gradient bound.} Since $\|\nabla f_i\|_\infty\le 1$, for any $x,y$,
$|f_i(x)-f_i(y)|\le \|x-y\|_1$. Thus rounding any $x^\star$ to its nearest grid point $g(x^\star)$ changes $f_i$ by at most $\|x^\star-g(x^\star)\|_1\le 2/k$.
This yields a discretization gap of at most $2/k$ in average reward.
\item \emph{Bandit optimization on the grid.} EXP3 with rewards in $[0,1]$ ensures (in expectation)
\[
\frac{1}{n}\Big(\max_{a\in\mathcal{A}}\sum_{i=1}^n f_i(a)-\sum_{i=1}^n f_i(x_i)\Big)
= O\!\left(\sqrt{\frac{K\log K}{n}}\right).
\]
\item \emph{Combine and optimize $k$.} Hence
\[
R_n \;\le\; \underbrace{\frac{2}{k}}_{\text{grid error}}
+\underbrace{C\sqrt{\frac{k^2\log k}{n}}}_{\text{EXP3 regret}}
\]
for an absolute constant $C$. Choosing $k\asymp (n/\log n)^{1/4}$ gives
$R_n=\tilde{O}(n^{-1/4})$, which vanishes as $n\to\infty$.
\end{itemize}


\end{shaded}

\end{document}

