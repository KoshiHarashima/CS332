\documentclass[11pt]{article}
\usepackage[margin=.75in]{geometry}
\usepackage[all]{xy}

\usepackage{amsmath,amsthm,amssymb,color,latexsym}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{letterpaper}    
\usepackage{graphicx}
\usepackage{tabto}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}

\usepackage[noframe]{showframe}
\usepackage{framed}

\renewenvironment{shaded}{%
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{shadecolor}}%
  \MakeFramed{\advance\hsize-\width \FrameRestore\FrameRestore}}%
 {\endMakeFramed}
\definecolor{shadecolor}{gray}{0.90}

\newtheorem{problem}{Problem}

\newenvironment{solution}[1][\it{Solution:}]{\textbf{#1 } }

\onehalfspacing

\begin{document}
\begin{titlepage}
   \begin{center}
       \vspace*{9cm}

       \textbf{CS 332 Fall 2025}

       \vspace{0.5cm}
        Problem Set \#3
        \vfill

       \textbf{Koshi Harashima}\\
       Due Date: 10/17, 2025
            
   \end{center}
\end{titlepage}

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% Problems %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------------------
\begin{shaded}
\section*{Problem 2: Learning to predict}
\textbf{Conclusion.} Exponential Weights on a $k$-level grid with learning rate $\eta=\tfrac{1}{2}$ yields
\[
R_n \;\le\; \frac{2\log k}{n}+\frac{1}{4k^2}.
\]
Choosing $k\asymp \sqrt{n/\log n}$ gives $R_n=O((\log n)/n)$.

\paragraph{Algorithm (EWA on $k$ fixed predictors).}
Discretize $[0,1]$ to $k$ points $\mathcal{P}=\{0,\tfrac{1}{k-1},\dots,1\}$.
Initialize $w_1(p)=1$ for all $p\in\mathcal{P}$. For days $t=1,\dots,n$:
\begin{enumerate}
\item Form normalized weights $q_t(p)=\frac{w_t(p)}{\sum_{u\in\mathcal{P}}w_t(u)}$.
\item Predict the mixture $p_t=\sum_{p\in\mathcal{P}} q_t(p)\,p$.
\item After seeing $\theta_t\in\{0,1\}$, update
\[
w_{t+1}(p)=w_t(p)\,\exp\!\Big(-\tfrac{1}{2}(p-\theta_t)^2\Big).
\]
\end{enumerate}

\paragraph{Why it works (short).}
\begin{itemize}
\item \emph{Mixability bound.} For squared loss on $[0,1]$, $\eta=\tfrac12$ is valid; EWA guarantees
\[
\sum_{t=1}^n (p_t-\theta_t)^2 \;\le\; \min_{p\in\mathcal{P}} \sum_{t=1}^n (p-\theta_t)^2 \;+\; 2\log k.
\]
Dividing by $n$ gives $\frac{2\log k}{n}$ regret to the best grid predictor.
\item \emph{Rounding error to the best fixed $p\in[0,1]$.} Let $p^\star=\arg\min_{p\in[0,1]}\frac1n\sum_{t=1}^n(p-\theta_t)^2=\bar{\theta}$. Rounding $p^\star$ to the nearest grid point $q\in\mathcal{P}$ changes the average loss by $(q-p^\star)^2\le \tfrac{1}{4k^2}$ (since the squared loss is quadratic and the derivative term cancels at $p^\star$).
\item \emph{Combine and optimize $k$.} Hence $R_n\le \frac{2\log k}{n}+\frac{1}{4k^2}$. Setting $k\asymp \sqrt{n/\log n}$ yields $R_n=O((\log n)/n)\to 0$.
\end{itemize}


\end{shaded}

\end{document}